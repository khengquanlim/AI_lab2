{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of lab2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XANaLmqSfsen"
      },
      "source": [
        "# Task 1: Three-layer feed-forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjVFnmlQrhiP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time \n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.io import loadmat\n",
        "import pdb\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "from PIL import Image\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0icaXfePH5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c430dc-c0ce-4499-85d1-993c31f09c9f"
      },
      "source": [
        "device= torch.device(\"cuda\")\n",
        "#device= torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZrUguc5xuEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b57bdb-2f2c-439e-feeb-26526ae76695"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtyOKMb2PNju"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = 2065\n",
        "hidden_size = 500\n",
        "mbatch_size = 64\n",
        "TRAIN_SPLIT = 0.7\n",
        "\n",
        "#epoch for now, set 2000/more for better answer\n",
        "num_epochs = 3000\n",
        "learning_rate = 0.001\n",
        "\n",
        "histories={}\n",
        "# DECLARATION OF CONSTANT VALUES\n",
        "\n",
        "# for training of model\n",
        "EPOCH_STARTER = 500 # print every 500 epoch for training of the models\n",
        "\n",
        "# for plot size\n",
        "WIDTH = 20\n",
        "HEIGHT = 10\n",
        "\n",
        "# optimizer\n",
        "OPTIMIZERS = ['sgd']\n",
        "\n",
        "# base path\n",
        "COLAB_FILEPATH = '/gdrive/MyDrive/secondhalf_LAB/OQC.mat'\n",
        "\n",
        "# sub-path\n",
        "SUBPATH = 'saved_weights/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvTcJcxs0Tj4"
      },
      "source": [
        "# scale data, delete if not necessary, try\n",
        "def scale(X, X_min, X_max):\n",
        "    return (X - X_min)/(X_max-X_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw8xIkGeSliN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a077f4d-bc75-4077-abed-1c501d29c5d9"
      },
      "source": [
        "#load matlab file \n",
        "oqc_dataset = loadmat(COLAB_FILEPATH)\n",
        "oqc_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__globals__': [],\n",
              " '__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Sun Mar 29 14:29:37 2020',\n",
              " '__version__': '1.0',\n",
              " 'data': array([[0.39052473, 0.14779758, 0.1563981 , ..., 0.07962529, 0.92246927,\n",
              "         2.        ],\n",
              "        [0.39052473, 0.14779758, 0.1563981 , ..., 0.05367681, 0.92148159,\n",
              "         2.        ],\n",
              "        [0.5084938 , 0.14779758, 0.1563981 , ..., 0.05611241, 0.9204939 ,\n",
              "         2.        ],\n",
              "        ...,\n",
              "        [0.54001505, 0.00982888, 0.77251185, ..., 0.05611241, 0.93135813,\n",
              "         0.        ],\n",
              "        [0.60664394, 0.00982888, 0.77251185, ..., 0.05658079, 0.93086416,\n",
              "         0.        ],\n",
              "        [0.7189506 , 0.00982888, 0.77251185, ..., 0.05658079, 0.93086416,\n",
              "         0.        ]])}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4DMsHGOS-I-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a5397f-9982-47e3-ecdb-c817900604f1"
      },
      "source": [
        "data = oqc_dataset['data']\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2952, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gC9TynNmBUH"
      },
      "source": [
        "# assign the corresponding part of the dataset to the predictor(X) and response(Y) variables \n",
        "X_raw, Y_raw = data[1:, :48].astype(np.float32), data[1:,-1].astype(np.long)\n",
        "X = scale(X_raw, np.min(X_raw, axis=0), np.max(X_raw, axis=0))\n",
        "\n",
        "Y = Y_raw.reshape(Y_raw.shape[0], 1)\n",
        "#Y = Y.astype(np.long)\n",
        "\n",
        "#perform train-test-split of the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=TRAIN_SPLIT, shuffle=True, random_state=42)\n",
        "X_train = torch.tensor(X_train)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "n, c = X_train.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11-o8Rb-tTie"
      },
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "    \"\"\"TensorDataset with support of transforms.\n",
        "    \"\"\"\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "        self.tensors = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.tensors[0][index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].size(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poc9Xs58th2a"
      },
      "source": [
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "\n",
        "test_dataset_normal = CustomTensorDataset(tensors=(X_test, y_test), transform=None)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset_normal, batch_size=mbatch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2A19b-K4LKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c13b535-1393-4d32-b085-56c0a318dbfd"
      },
      "source": [
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=48, out_features=8, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=8, out_features=8, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=8, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwpQGcMK4VYz"
      },
      "source": [
        "# Loss and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMBoVAX_aDpE"
      },
      "source": [
        "test_loss_plot = list()\n",
        "correct=0\n",
        "total=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRlRphcYgpad"
      },
      "source": [
        "# Train the model\n",
        "#time0 = time.time()\n",
        "def trainmodel(num_epochs):\n",
        "  correct_count, all_count = 0, 0\n",
        "  print(num_epochs)\n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs): \n",
        "    \n",
        "      for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "        loss = criterion(outputs, targets.squeeze(dim=1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #_, predicted = outputs.max(1)\n",
        "        #total += targets.size(0)\n",
        "        #correct += predicted.eq(targets).sum().item()\n",
        "        #rawPredicted, predicted = torch.max(F.softmax(inputs.data,dim=1), 1)\n",
        "        #correct       = (predicted == targets).sum().item()\n",
        "        #accuracy      = 100*correct/(predicted == targets).shape[0] \n",
        "        # ps = torch.exp(logps).cpu()\n",
        "        # probab = list(ps.detach().numpy()[0])\n",
        "        # pred_target = probab.index(max(probab))\n",
        "        # true_target = targets.cpu().numpy()[i]\n",
        "        # if(true_target == pred_target):\n",
        "        #   correct_count += 1\n",
        "        # all_count += 1\n",
        "      \n",
        "      # _, predicted = outputs.max(1)\n",
        "        #total += targets.size(0)\n",
        "        #correct += predicted.eq(targets).sum().item()\n",
        "        #ps = torch.exp(logps).cpu()\n",
        "        #probab = list(ps.detach().numpy()[0])\n",
        "        #pred_target = probab.index(max(probab))\n",
        "        #true_target = targets.cpu().numpy()[i]\n",
        "        #if(true_target == pred_target):\n",
        "        #  correct_count += 1\n",
        "        #all_count += 1\n",
        "      \n",
        "        \n",
        "      if (epoch+1) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "        \n",
        "      \n",
        "      test_loss_plot.append(loss.item())\n",
        "      epochs_count = [i for i in range(num_epochs)]\n",
        "\n",
        "  correct_count, all_count = 0, 0\n",
        "  running_error = 0\n",
        "  correct=0\n",
        "  total=0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(test_loader):\n",
        "      for i in range(len(targets)):\n",
        "        \n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "        logps = model(inputs)\n",
        "\n",
        "        ps = torch.exp(logps).cpu()\n",
        "        probab = list(ps.detach().numpy()[0])\n",
        "        pred_target = probab.index(max(probab))\n",
        "        true_target = targets.cpu().numpy()[i]\n",
        "        if(true_target == pred_target):\n",
        "          correct_count += 1\n",
        "        all_count += 1\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        rawPredicted, predicted = torch.max(F.softmax(inputs.data,dim=1), 1)\n",
        "        correct       = (predicted == targets).sum().item()\n",
        "        accuracy      = 100*correct/(predicted == targets).shape[0] \n",
        "\n",
        "\n",
        "  print(\"Number Of Images Tested =\", all_count)\n",
        "  print(\"\\nModel Accuracy =\", (correct_count/all_count) * 100)\n",
        "  print(\"Accuracy =\", accuracy )\n",
        "  #accu=100.*correct/total\n",
        "  #print('Accuracy: %.3f'%(accu))\n",
        "\n",
        "  #print(\"Number Of Images Tested =\", all_count)\n",
        "  #print(\"\\nModel Accuracy =\", (correct_count/all_count) * 100)\n",
        "  #print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IjZHavYzvRcb",
        "outputId": "7d519c72-3fba-4575-da9d-82608b0db8e7"
      },
      "source": [
        "epochlist = [1, 5, 10, 20, 30, 50, 100, 150, 200, 500, 1000, 1500, 2000]\n",
        "\n",
        "for i in epochlist:\n",
        "  trainmodel(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 29.45823927765237\n",
            "Accuracy = 1401.851851851852\n",
            "5\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 29.45823927765237\n",
            "Accuracy = 1401.851851851852\n",
            "10\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 29.45823927765237\n",
            "Accuracy = 1401.851851851852\n",
            "20\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 29.45823927765237\n",
            "Accuracy = 1401.851851851852\n",
            "30\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 33.29571106094808\n",
            "Accuracy = 1401.851851851852\n",
            "50\n",
            "Epoch [50/50], Loss: 1.1122\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 35.55304740406321\n",
            "Accuracy = 1401.851851851852\n",
            "100\n",
            "Epoch [50/100], Loss: 1.1112\n",
            "Epoch [100/100], Loss: 1.1003\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 37.24604966139955\n",
            "Accuracy = 1401.851851851852\n",
            "150\n",
            "Epoch [50/150], Loss: 1.0918\n",
            "Epoch [100/150], Loss: 1.0769\n",
            "Epoch [150/150], Loss: 1.0468\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 37.24604966139955\n",
            "Accuracy = 1401.851851851852\n",
            "200\n",
            "Epoch [50/200], Loss: 0.9867\n",
            "Epoch [100/200], Loss: 0.8835\n",
            "Epoch [150/200], Loss: 0.7479\n",
            "Epoch [200/200], Loss: 0.6197\n",
            "Number Of Images Tested = 886\n",
            "\n",
            "Model Accuracy = 34.76297968397291\n",
            "Accuracy = 1401.851851851852\n",
            "500\n",
            "Epoch [50/500], Loss: 0.5216\n",
            "Epoch [100/500], Loss: 0.4473\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-fd6901d77424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrainmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-c9b23320c2a7>\u001b[0m in \u001b[0;36mtrainmodel\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHhH3boPcImr"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "#np.arrays(epoch_count)\n",
        "#np.arrays(test_loss_plot)\n",
        "plt.plot(epochs_count, test_loss_plot ,'green',label='test')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AondrBCxjEKh"
      },
      "source": [
        "# correct_count, all_count = 0, 0\n",
        "# running_error = 0\n",
        "# model.eval()\n",
        "\n",
        "# correct=0\n",
        "# total=0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   for i, (inputs, targets) in enumerate(train_loader):\n",
        "#     for i in range(len(targets)):\n",
        "      \n",
        "#       inputs = inputs.cuda()\n",
        "#       targets = targets.cuda()\n",
        "\n",
        "#       logps = model(inputs)\n",
        "\n",
        "#       ps = torch.exp(logps).cpu()\n",
        "#       probab = list(ps.detach().numpy()[0])\n",
        "#       pred_target = probab.index(max(probab))\n",
        "#       true_target = targets.cpu().numpy()[i]\n",
        "#       if(true_target == pred_target):\n",
        "#         correct_count += 1\n",
        "#       all_count += 1\n",
        "#       _, predicted = outputs.max(1)\n",
        "#       total += targets.size(0)\n",
        "#       correct += predicted.eq(targets).sum().item()\n",
        "#       rawPredicted, predicted = torch.max(F.softmax(inputs.data,dim=1), 1)\n",
        "#       correct       = (predicted == targets).sum().item()\n",
        "#       accuracy      = 100*correct/(predicted == targets).shape[0] \n",
        "\n",
        "\n",
        "# print(\"Number Of Images Tested =\", all_count)\n",
        "# print(\"\\nModel Accuracy =\", (correct_count/all_count) * 100)\n",
        "# print(\"Accuracy =\", accuracy )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cK72zcpr0R6"
      },
      "source": [
        "correct_count, all_count = 0, 0\n",
        "running_error = 0\n",
        "model.eval()\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (inputs, targets) in enumerate(test_loader):\n",
        "    for i in range(len(targets)):\n",
        "      \n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "\n",
        "      logps = model(inputs)\n",
        "\n",
        "      ps = torch.exp(logps).cpu()\n",
        "      probab = list(ps.detach().numpy()[0])\n",
        "      pred_target = probab.index(max(probab))\n",
        "      true_target = targets.cpu().numpy()[i]\n",
        "      if(true_target == pred_target):\n",
        "        correct_count += 1\n",
        "      all_count += 1\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += targets.size(0)\n",
        "      correct += predicted.eq(targets).sum().item()\n",
        "      rawPredicted, predicted = torch.max(F.softmax(inputs.data,dim=1), 1)\n",
        "      correct       = (predicted == targets).sum().item()\n",
        "      accuracy      = 100*correct/(predicted == targets).shape[0] \n",
        "\n",
        "\n",
        "print(\"Number Of Images Tested =\", all_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count) * 100)\n",
        "print(\"Accuracy =\", accuracy )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhSswe-dAHSd"
      },
      "source": [
        "correct_count, all_count = 0, 0\n",
        "\n",
        "for i, (inputs, targets) in enumerate(test_loader):\n",
        "  for i in range(len(targets)):\n",
        "      with torch.no_grad():\n",
        "        # load data\n",
        "        inputs   = inputs.to(device)\n",
        "        targets  = targets.to(device)\n",
        "        ones        = torch.tensor([1.0])\n",
        "        targets  = targets.long()\n",
        "    \n",
        "        loss          = criterion(inputs,targets)\n",
        "      rawPredicted, predicted = torch.max(F.softmax(inputs.data,dim=1), 1)\n",
        "      correct       = (predicted == targets).sum().item()\n",
        "      accuracy      = 100*correct/(predicted == targets).shape[0]  # 1: correct, 0: wrong\n",
        "\n",
        "Accuracy = meanStd()\n",
        "Accuracy.calcMeanStd(accuracy)\n",
        "print(\"Number Of Images Tested =\", all_count)\n",
        "print('Accuracy: ',Accuracy.mean,'(+/-)',Accuracy.std)\n",
        "  \n",
        "  \n",
        "  # net = net.eval()\n",
        "  # with T.no_grad():\n",
        "    # raw_out = net(unknown)    # a Tensor\n",
        "  # pred_prob = raw_out.item()  # scalar, [0.0, 1.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-rQ0kw6GCO5"
      },
      "source": [
        "class meanStd(object):\n",
        "    def __init__(self):\n",
        "        self.mean     = 0.0\n",
        "        self.mean_old = 0.0\n",
        "        self.std      = 0.001\n",
        "        self.count    = 0.0\n",
        "        self.minMean  = 100.0\n",
        "        self.minStd   = 100.0\n",
        "        self.M_old    = 0.0\n",
        "        self.M        = 0.0\n",
        "        self.S        = 0.0\n",
        "        self.S_old    = 0.0\n",
        "        \n",
        "    def calcMeanStd(self, data, cnt = 1):\n",
        "        self.data     = data\n",
        "        self.mean_old = copy.deepcopy(self.mean)\n",
        "        self.M_old    = self.count*self.mean_old\n",
        "        self.M        = self.M_old + data\n",
        "        self.S_old    = copy.deepcopy(self.S)\n",
        "        if self.count > 0:\n",
        "            self.S    = self.S_old + ((self.count*data - self.M_old)**2)/(self.count*(self.count + cnt))\n",
        "        \n",
        "        self.count   += cnt\n",
        "        self.mean     = self.mean_old + np.divide((data-self.mean_old),self.count)\n",
        "        self.std      = np.sqrt(self.S/self.count)\n",
        "        \n",
        "        if (self.std != self.std).any():\n",
        "            print('There is NaN in meanStd')\n",
        "            pdb.set_trace()\n",
        "    \n",
        "    def resetMinMeanStd(self):\n",
        "        self.minMean = copy.deepcopy(self.mean)\n",
        "        self.minStd  = copy.deepcopy(self.std)\n",
        "        \n",
        "    def calcMeanStdMin(self):\n",
        "        if self.mean < self.minMean:\n",
        "            self.minMean = copy.deepcopy(self.mean)\n",
        "        if self.std < self.minStd:\n",
        "            self.minStd = copy.deepcopy(self.std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znJWUWjVflOR"
      },
      "source": [
        "# Task 2: Testing different network configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1WFx8zH80mC"
      },
      "source": [
        "## 2.1 Test for variables in hidden nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lDeqLp3gDRT"
      },
      "source": [
        "#Recall code for hidden node\n",
        "hidden_node = 512\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "#Reset list append\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af94oIbqv1B7"
      },
      "source": [
        "### Hidden nodes = 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCaso4av4Dl"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "# Loss and optimizer\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "\n",
        "num_epochs = 2000\n",
        "total_step = len(X_train)\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4FusexDv8MS"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[0] ,'green',label='hn = 128')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxKjT7wpsVo0"
      },
      "source": [
        "### Hidden nodes = 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUwSbnBerJlo"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "hidden_node = 256\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "\n",
        "test_loss_plot = list()\n",
        "num_epochs = 2000\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSuTOMwwFgbF"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[1] ,'green',label='hn = 256')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjMBGxXLshTh"
      },
      "source": [
        "### Hidden nodes = 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0aEaS7TtQnP"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "hidden_node = 512\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "\n",
        "num_epochs = 2000\n",
        "test_loss_plot = list()\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVSAtY-sajq"
      },
      "source": [
        "### Hidden nodes = 1024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6t0XHVXsZz3"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "hidden_node = 1024\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "\n",
        "num_epochs = 2000\n",
        "test_loss_plot = list()\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkCbblYitrhB"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[3] ,'green',label='hn = 1024')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVStntQuskRo"
      },
      "source": [
        "### Hidden nodes = 2048"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWP_JIPBsj_8"
      },
      "source": [
        " #Test for different hidden nodes\n",
        "hidden_node = 2048\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "\n",
        "num_epochs = 2000\n",
        "test_loss_plot = list()\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG_Qg9MXvCHC"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[4] ,'green',label='hn = 2048')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX1T3Rex46SV"
      },
      "source": [
        "### Comparing all different hidden nodes against test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmzx9qks48PT"
      },
      "source": [
        "# cross validation accuracy of different hidden neuron against epochs\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.plot(epochs_count,list_test_loss_plot[0],'yellow',label='hidden nodes = 128')\n",
        "plt.plot(epochs_count,list_test_loss_plot[1],'orange',label='hidden nodes = 256')\n",
        "plt.plot(epochs_count,list_test_loss_plot[2],'green',label='hidden nodes = 512')\n",
        "plt.plot(epochs_count,list_test_loss_plot[3],'blue',label='hidden nodes = 1024')\n",
        "plt.plot(epochs_count,list_test_loss_plot[4],'red',label='hidden nodes = 2048')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUEkvYD58_FF"
      },
      "source": [
        "## 2.2 Test for variables in hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwkwgjOy-6MC"
      },
      "source": [
        "### Hidden layers = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99UhXCMW-75P"
      },
      "source": [
        "#Recall code for hidden node\n",
        "\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "hidden_node = 512\n",
        "#Reset list append\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wDPRCsY--ly"
      },
      "source": [
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neLqgnRw--W-"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[0] ,'green',label='hd = 0')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbPIHhsH9Csw"
      },
      "source": [
        "### Hidden layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTgRZvK-9BTX"
      },
      "source": [
        "#Recall code for hidden node\n",
        "\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "hidden_node = 512\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNAlC9t29UMr"
      },
      "source": [
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWobgdkT9dC6"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[1] ,'green',label='hd = 1')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cU2rvc9jZe"
      },
      "source": [
        "### Hidden layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTn-tjrO9mFp"
      },
      "source": [
        "#Recall code for hidden node\n",
        "\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "hidden_node = 512\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrGsbGH9rfm"
      },
      "source": [
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nnn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAQTxQyi90yd"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[2] ,'green',label='hd = 2')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiTXW4kC99N9"
      },
      "source": [
        "### Hidden layers = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHCWHB7U-AjB"
      },
      "source": [
        "#Recall code for hidden node\n",
        "\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "hidden_node = 512\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7NntOOq-DfK"
      },
      "source": [
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPCz04BM-EaV"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[3] ,'green',label='hd = 3')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtrpxYzA-Ja0"
      },
      "source": [
        "### Hidden layers = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuLfrxzD-HqI"
      },
      "source": [
        "#Recall code for hidden node\n",
        "\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "hidden_node = 512\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb0cSDOa-zR3"
      },
      "source": [
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 2000\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YUEcdh1-16j"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[4] ,'green',label='hd = 4')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWyxaSD_69D"
      },
      "source": [
        "### Comparing all different hidden layers against test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q3Imbxp_-iY"
      },
      "source": [
        "# cross validation accuracy of different hidden neuron against epochs\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.plot(epochs_count,list_test_loss_plot[0],'yellow',label='hidden layers = 0')\n",
        "plt.plot(epochs_count,list_test_loss_plot[1],'orange',label='hidden layers = 1')\n",
        "plt.plot(epochs_count,list_test_loss_plot[2],'green',label='hidden layers = 2')\n",
        "plt.plot(epochs_count,list_test_loss_plot[3],'blue',label='hidden layers = 3')\n",
        "plt.plot(epochs_count,list_test_loss_plot[4],'red',label='hidden layers = 4')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meg-3Qn8Jcu5"
      },
      "source": [
        "# Task 3: Testing different learning rates and setting adaptive learning rate to different epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlX6LuO4Nh03"
      },
      "source": [
        "## 3.1 Experiment with different learning rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxtGmoiIHdto"
      },
      "source": [
        "### Learning rate = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E1PFJo4Hhs3"
      },
      "source": [
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "\n",
        "#Reset list append\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFgzkdNcHjFh"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 0\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHXG9eFAHm_S"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[0] ,'green',label='lr = 0')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPNr08qyFguX"
      },
      "source": [
        "### Learning rate = 10**(-1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SMSLZpyGpQu"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-1)\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOJ3FhhDGu1b"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[1] ,'green',label='lr = 10**(-1)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gGoa_KoHAml"
      },
      "source": [
        "### Learning rate = 10**(-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2kBwuRZHGuI"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-2)\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfvdJ5L1HJ1R"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[2] ,'green',label='lr = 10**(-2)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOjEqXG6HLtJ"
      },
      "source": [
        "### Learning rate = 10**(-3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZWVDXPSHNWf"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-3)\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1c7JueHPVk"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[3] ,'green',label='lr = 10**(-3)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y3A7x0uIIuX"
      },
      "source": [
        "### Learning rate = 10**(-4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_06TFubCIIQt"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-4)\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wT34VWpIQmo"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[4] ,'green',label='lr = 10**(-4)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-dQmmezIZ7V"
      },
      "source": [
        "### Comparing all different learning rates against test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlTzuJweIczT"
      },
      "source": [
        "# cross validation accuracy of different hidden neuron against epochs\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.plot(epochs_count,list_test_loss_plot[0],'yellow',label='lr = 0')\n",
        "plt.plot(epochs_count,list_test_loss_plot[1],'orange',label='lr = 10**(-1)')\n",
        "plt.plot(epochs_count,list_test_loss_plot[2],'green',label='lr = 10**(-2)')\n",
        "plt.plot(epochs_count,list_test_loss_plot[3],'blue',label='lr = 10**(-3)')\n",
        "plt.plot(epochs_count,list_test_loss_plot[4],'red',label='lr = 10**(-4)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sqmjPDvI633"
      },
      "source": [
        "## 3.2 Experimenting with optimal learning rate but different epochs\n",
        "\n",
        "### We can take learning rate = 10**(-1) and change the epochs to decide the adaptive learning rates to number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji3nUD7fJXA1"
      },
      "source": [
        "#Reset list append\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-1)\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 3000 later\n",
        "num_epochs = 3000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZEZnl6FNx1W"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[0] ,'green',label='Epoch = 2000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrRWgUK3OCot"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-1)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 4000 later\n",
        "num_epochs = 4000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "total_step = len(X_train)\n",
        "for epoch in range(num_epochs): \n",
        "    for i in range((n - 1) // mbatch_size + 1):\n",
        "      start_i = i * mbatch_size\n",
        "      end_i = start_i + mbatch_size\n",
        "      inputs = torch.from_numpy(X_train[start_i:end_i])\n",
        "      targets = torch.from_numpy(y_train[start_i:end_i])\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "    \n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3jTiPTdOA2I"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[1] ,'green',label='Epoch = 3000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqd9B9C-OLkH"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-1)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 5000 later\n",
        "num_epochs = 5000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "total_step = len(X_train)\n",
        "for epoch in range(num_epochs): \n",
        "    # Convert numpy arrays to torch tensors\n",
        "    inputs = torch.from_numpy(X_train)\n",
        "    targets = torch.from_numpy(y_train)\n",
        "        \n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "        \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "    test_loss_plot.append(loss.item())\n",
        "\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aclraZQTOVnp"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[2] ,'green',label='Epoch = 4000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-xB8qnfOYKn"
      },
      "source": [
        "#Test for different hidden nodes\n",
        "learning_rate = 10**(-1)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "#For test, put to 6000 later\n",
        "num_epochs = 6000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "total_step = len(X_train)\n",
        "for epoch in range(num_epochs): \n",
        "    # Convert numpy arrays to torch tensors\n",
        "    inputs = torch.from_numpy(X_train)\n",
        "    targets = torch.from_numpy(y_train)\n",
        "        \n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "        \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "    test_loss_plot.append(loss.item())\n",
        "\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn99-wJ-Oblw"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[3] ,'green',label='Epoch = 2000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msofMt6BOsMI"
      },
      "source": [
        "# cross validation accuracy of different hidden neuron against epochs\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.plot(epochs_count,list_test_loss_plot[0],'yellow',label='Epoch = 2000')\n",
        "plt.plot(epochs_count,list_test_loss_plot[1],'orange',label='Epoch = 3000')\n",
        "plt.plot(epochs_count,list_test_loss_plot[2],'green',label='Epoch = 4000')\n",
        "plt.plot(epochs_count,list_test_loss_plot[3],'blue',label='Epoch = 2000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWOqYB_-XzLu"
      },
      "source": [
        "# Task 4: Testing different mini-batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuQRlnZxjO_w"
      },
      "source": [
        "### Mini-batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLVEkMCOjLEZ"
      },
      "source": [
        "#Recall code for hidden node\n",
        "\n",
        "# Fully 3-layer connected neural network with one hidden layer. Output includes as 1 layer\n",
        "# Edit here to change 1. Hidden layer by adding nn.linear, 2. Change 512 (hidden nodes)\n",
        "hidden_node = 512\n",
        "#Reset list append\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(48, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, hidden_node),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHVPxKYjjXpf"
      },
      "source": [
        " #Test for different batch size\n",
        "mbatch_size = 1\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      #loss = criterion(outputs, torch.max(targets, 1)[1])\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vX0pxXDjZ4Q"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[0] ,'green',label='mbs = 1')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EwTmhNzYJCY"
      },
      "source": [
        "### Mini-batch size = 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0NqIl6Ag4n3"
      },
      "source": [
        " #Test for different batch size\n",
        "mbatch_size = 8\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "list_test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUMlWENjiMS-"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[1] ,'green',label='Mini-batch size = 8')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdnbeE23hCpZ"
      },
      "source": [
        "### Mini-batch size = 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uanjbCd0hE0F"
      },
      "source": [
        " #Test for different batch size\n",
        "mbatch_size = 16\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2gC2UF_jELl"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[2] ,'green',label='mbs = 16')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBLxaI3ahL2z"
      },
      "source": [
        "### Mini-batch size = 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9AIjo4bhL20"
      },
      "source": [
        " #Test for different batch size\n",
        "mbatch_size = 32\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVSKb6IGjHJ2"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[3] ,'green',label='mbs = 32')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFviJ1zchMWl"
      },
      "source": [
        "### Mini-batch size = 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY8GI7JlhMWm"
      },
      "source": [
        " #Test for different batch size\n",
        "mbatch_size = 64\n",
        "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=mbatch_size)\n",
        "# Loss and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#For test, put to 2000 later\n",
        "num_epochs = 2000\n",
        "#Reset test_loss_plot\n",
        "test_loss_plot = list()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): \n",
        "  \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets.squeeze())\n",
        "        \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "                \n",
        "    if (epoch+1) % 50 == 0:\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "    test_loss_plot.append(loss.item())\n",
        "    epochs_count = [i for i in range(num_epochs)]\n",
        "list_test_loss_plot.append(test_loss_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXkg0tv6jiPl"
      },
      "source": [
        "# Plot the graph, find a better graph, optimally the loss aginst epoch graph, try to integrate\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.plot(epochs_count, list_test_loss_plot[4] ,'green',label='mbs = 64')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_3NxNXzjlfD"
      },
      "source": [
        "### Comparing all different mini-batch size against test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4dIrbcUjpqw"
      },
      "source": [
        "# cross validation accuracy of different hidden neuron against epochs\n",
        "plt.figure(figsize=(WIDTH,HEIGHT))\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.plot(epochs_count,list_test_loss_plot[0],'yellow',label='mbs = 8')\n",
        "plt.plot(epochs_count,list_test_loss_plot[1],'orange',label='mbs = 16')\n",
        "plt.plot(epochs_count,list_test_loss_plot[2],'green',label='mbs = 32')\n",
        "plt.plot(epochs_count,list_test_loss_plot[3],'blue',label='mbs = 64')\n",
        "plt.plot(epochs_count,list_test_loss_plot[4],'red',label='mbs = 128')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}